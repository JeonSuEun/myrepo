밀도 추정
어떤 점 x에서 데이터가 발생할 확률, 즉 확률분포 P(x)를 구하는 문제

커널 밀도 추정
1. 히스토그램 방법 => 특징 고간을 칸의 집합으로 분할한 다음, 칸에 있는 샘플의 빈도를 세어 추정 P(x) = bin(x)/n
문제점: 매끄럽지 못하고 계단 모양을 띠는 확률밀도함수가 됨, 칸의 크기와 위치에 민감함

2. 커널 밀도 추정법 => 커널을 씌우고 커널 안에 있는 샘플의 가중 합을 이용 / 대역폭 h의 크기가 중요 / 커널 넓이 => 1
매끄러운 확률밀도함수를 추정 / h가 너무 작으면 뾰족뾰족한 모양, 너무 크면 뭉개짐, 적절하게 설정해야 함
근본적인 문제점: 샘플을 모두 저장하고 있어야 하는 메모리 기반 방법(메모리를 많이 쓴다), 데이터 희소성(차원의 저주)(데이터가 낮은 차원인 경우로 국한하여 활용)

3. 가우시안 혼합 => 데이터가 가우시안 분포를 따른다고 가정하고 평균 벡터와 공분산 행렬을 추정
대부분의 데이터가 하나의 가우시안으로 불충분
주어진 데이터: 훈련집합 x...., 가우시안의 개수 k , 매겨변수... 
최대 우도를(최대가능성) 이용한 최적화 문제로 공식화 

4. EM 알고리즘(가우시안 혼합을 위한)

공간 변환
비지도 학습을 이용하여 최적의 공간 변환을 자동으로 알아내야 함
인코딩(원래 공간을 다른 공간으로 변환) / 디코딩(변환 공간을 원래 공간으로 역변환)
데이터 압축의 경우, 역변환을 얻은 x는 원래 신호 x와 가급적 같아야 함 / 데이터 가시화에서는 디코딩 불필요

----------------------------------------------------------------------------------------------------------------

선형 인자 모델
선형 연산을 이용한 공간 변환 기법

1.(1) 주성분 분석(PCA)
데이터의 특징을 유지하며 차원을 축소할 수 있는 가장 대표적인 차원 축소 알고리즘(데이터를 새로운 직교 좌표계로 옮겨 놓는 선형 차원 축소)
데이터에 가장 가까운 평면인 초평면을 찾아 데이터를 투영시키는 것
데이터의 특징을 잘 보존하는 투영면을 찾기 위해 분산을 가장 잘 유지하는 축을 찾아,이들을 이용하여 투영면을 구성하는 것

n차원 데이터 m개를 m x n 행렬로 표현한 이 행렬의 공분산행렬을 구해 고유값 분해 실시(공분산 행렬은 고유벡터로 이루어진 행렬 W와 고유 값을 가진 대각행렬A?로 분해)(이때 W를 구성하는 기저 벡터,즉 각 열 벡터들이 주성분)
M의 분산을 최대로 보존하면서 k차원으로 축소시킨 Mk를 얻는 방법은 주성분들 중 중요한 k개의 주성분을 기저로 하는 공간으로 옮기면 됨
주성분 분석은 특이값 분해(SVD)(mxn을 분해해서 쉽게 행렬(계산하기 쉽게))라 부르는 방법으로 구현하는 것이 일반적
특이값 분해를 통해 얻은 V가 바로 주성분 벡터, 즉 공분산의 고유벡터로 구성된 W와 동일

특이값 분해 => 행렬 M의 변환을 회전, 크기변경, 다시 회전으로 표현하는 것
행렬 M의 이러한 공간의 가중합으로 표현하는 것(가중치는 특이값,이 특이값이 클수록 중요한 데이터의 정보를 담고있는 공간)
주성분을 찾는 방법은 특이값 분해를 하여 얻은 VT의 전치인 V의 열을 가져오는 것

데이터를 원점 중심으로 옮기는 전처리 먼저 수행 
u (1)-> x 
  (0)-> y  옆 예시는 x축으로 투영한것
목적: 손실을 최소화하면서 저차원으로 변환하는 것 
변환된 훈련집합의 분산이 클수록 정보 손실이 적다고 판단!

1.(2) 커널 PCA
서포트 벡터 머신(SVM)을 다루면서 커널 트릭을 사용, 이것은 특징 공간을 다항화하지 않고도 비슷한 효과를 얻을 수 있는 방법(커널 트릭은 주성분 분석에도 똑같은 방식으로 적용이 가능)(EX-> 다항,선형,방사기저함수,시그모이드,코사인 커널)



2. 독립 성분 분석(ICA) 
블라인드 원음 분리 문제(여러 신호가 섞여 나타남)
독립성 가정: 원래 신호가 서로 독립이라는 가정
비가우시안 가정: 원래 신호가 가우시안이라면 혼합 신호도 가우시안이 되므로 분리할 실마리 없음(그러므로 비가우시안이면 실마리가 있음)
ICA의 문제 풀이 => 원래 신호의 비가우시안인 정도를 최대화하는 가중치를 구하는 전략 사용
ICA 학습 => 전처리 수행 -> 최적 가중치 구함

PCA와 ICA 비교
ICA는 비가우시안과 독립성 가정, PCA는 가우시안과 비상관을 가정 
ICA는 4차 모멘트까지 사용, PCA는 2차 모멘트까지 사용
ICA로 찾은 축은 수직 아님, PCA로 찾은 축은 서로 수직
ICA는 주로 블라인드 원음 분리 문제를 푸는데, PCA는 차원 축소 문제를 품

3. 희소 코딩 => 기저 함수 또는 기저 벡터의 선형 결합으로 신호를 표현(푸리에 변환, 웨이블릿 변환)
사전 D를 구성하는 기저 벡터의 선형 결합으로 신호 x를 표현
다른 변환 기법과 다른점 => 비지도 학습이 사전(기저벡터)를 자동으로 알아냄(희소코딩은 데이터에 맞는 기저벡터 사용하는 셈), 사전의 크기를 과잉 완벽하게 책정, 희소코드a를 구성하는 요소 대부분이 0 값을 가짐
희소 코딩 구현 => 최적의 사전과 최적의 희소 코드를 알아내야 함, 희소코드의 희소성을 강제하는 규제항 존재

-----------------------------------------------------------------------------------------------------------------

매니폴드 학습
매니폴드 => 매니폴드는 고차원 공간에 내재한 저차원 공간(이웃 점들 사이의 관계를 유클리드 공간으로 표현할 수 있는 공간)
매니폴드 가정 => 고차원 공간에 주어진 실제 세계의 데이터는 고차원 입력 공간에 내재한 훨씬 저차원인 차원 매니폴드의 인근에 집중되어 있다.

1. IsoMap
각 샘플에 대해 가장 가까운 샘플을 연결하여 그래프를 만들어 측지선거리의 대소 관계를 유지하며 차원을 축소하는 기법
알고리즘 => 최근접 이웃 그래프 구축 / 행렬M의 고유 벡터를 계산하고, 큰 순서대로 d(low)개의 고유 벡터를 선택 
M의 크기가 방대한 문제점

2. LLE(국소적 선형 임베딩)
인접한 데이터들이 차지하는 작은 영역을 선형관계로 보고 이러한 관계를 잘 유지하는 임베딩 방법 => 이 선형관계를 그대로 유지하는 임베딩곤가을 찾아 데이터를 옮겨 놓는 것
LLE 알고리즘 : (1) 모든 점에 대해 이웃 지점들을 찾음 -> (2) 모든 점에 대해 이웃 지점들과의 선형 관계를 찾음 -> (3) 앞에서 찾은 선형관계가 유지되는 임베딩 공간을 찾음

거리 행렬 M 대신에 함수를 최소로 하는 가중치 행렬W를 사용
저차원 공간에서는, 변환된 저차원 공간의 점을 (X` ={y1,y2,...}) 최소화하는 X`를 찾아야함
고차원 원래 공간에서의 식과 저차원 변환 공간에서 식을 비슷하게 유지함으로써 원래 데이터X와 변환된 데이터 X`가 비슷한 구조를 가짐

3. t-SNE(시각화 했을 때 가장 잘 보임)
비슷한 데이터 인스턴스들끼리 가까운 거리를 유지하면서, 낮은 차원의 임베딩 공간을 찾는 분석
매니폴드 공간 변환 기법 중에서 가장 뛰어남
원래 공간에서 유사도 측정 / 변환된 공간에서의 유사도는 스튜던트 t 분포로 측정
원래 데이터와 변환된 데이터의 구조가 비슷해야 하므로 확률 분포가 서로 비슷할수록 좋음(비슷한 정도 측정위해 KL 다이버전스 사용)
학습 알고리즘: 목적함수 J를 최소로 하는, 즉 P와 Q의 KL 다이버전스를 최소로 하는 X`를 찾는 문제

-----------------------------------------------------------------------------------------------------------------

신경망 기초

인공 신경망은 이러한 신경세포의 동작을 흉내 내는 장치나 소프트웨어를 만들어 뇌가 수행하는 인지나 사고 능력을 갖춘 기계를 만들려는 노력(이러한 방식의 연구를 통해 인공지능을 구현하려는 방식을 연결주의)
워렌 맥컬록,월터 피츠 => 삼각형은 신경세포,이들이 활성화되면 선으로 표현된 연결의 끝 지점에 있는 시냅스가 이 신호를 다른 신경세포로 전달, 파란색을 가진 끝점은 1의 신호가 전달되는 것을 의미 / 흰색원은 억제~
, 학습이 불가능함(미리 설계만 가능)

퍼셉트론(프랭크 로젠블랫) => 위 예시에서 학습이 가능하게 만든 것!
연결될 때 연결강도에 의해 조정되어 전달 / 합산된 신호에 따라 출력을 결정하는 활성화함수가 최종 출력을 발생
학습은 경험을 통해 기능이나, 성능이 바뀌는 것(이것은 연결강도를 바꾸어 나감)
활성화함수 => 출력 노드에 모아진 신호를 다음으로 내어 보낼 때 얼마나 강하게 보낼지를 결정하는 함수(계단함수는 미분되는 구간에서 언제나 0이기때문에 새롭게 도입된 함수가 시그모이드)
오차를 줄이는 방향으로 연결강도를 조정하는 일이 신경망의 최적화 / 하이퍼파라미터로 학습과정 조절

학습의 원리 - 연결강도의 변경
어떻게 연결강도를 변경할 것인지 중요한 토대를 만든 이 => 도널드 헵(함께 활성화되는 세포는 함께 연결된다) (함께 연결된다는 것은 연결의 강도가 커진다는 것)
헵의 발견은 두뇌 신경망의 기능이 고정되어 있지 않고 바뀔 수 있는 가소성을 가지고 있음을 설명
헵의 학습 법칙은 신경세포에 가해지는 입력과 출력에 따라 새로운 가중치를 바꿔나가는 것
가중치의 갱신도 학습률을 통해 조금씩 이루어지게 만듬
이항 불리언 연산이 가능하도록 하는 퍼셉트론 => 참 or 거짓을 갖는 입력은 x1과 x2에 제공, 이 신호는 가중치가 곱해져 출력노드로,
이때 신호의 편향이 이루어지도록 하는 가중치 b가 추가(편향은 신호값을 양의 방향이나 음의 방향으로 이동시키는 역할)
가장 잘 알려진문제는 퍼셉트론이 XOR을 구현 X
방법들: 선 2개, 커널, 특징변환 ...

퍼셉트론이 XOR문제를 풀게 하는 법
입력과 출력을 바로 연결하지 않고 중간에 신호를 중계하는 신경세포들을 두는 것(이런 중간 노드들의 계층을 은닉층)
다층 퍼셉트론 구현 가능 => 경사 하강법을 사용하여 구현(시그모이드 or 로지스틱)
오차를 아래로 전파하자(다층 퍼셉트론의 학습) 
역전파 알고리즘 => 미분 y`가 연결망을 거꾸로 타고 전달되어 가중치를 조정하는 것
연결강도w를 갱신하는 방법을 연결강도u에도 전파할 수 있으며 이것이 오차 역전파의 기본 개념
순전파 => 입력의 신호가 연결강도로 증폭되고 활성화 함수를 통과하여 출력 노드로 전달되는 과정
오차역전파 => 출력단의 오차가 역방향으로 활성화 함수의 미분 함수를 거친 뒤에 연결강도에 의해 증폭되어 입력단까지 전달되는 과정

역전파 알고리즘을 적용하는 조건 => 1. 오차를 계산하는 방법이 존재 2. 활성화 함수는 미분이 가능한 함수
역전파 방법은 계속해서 아래 계층으로 내려갈 수 있음

퍼셉트론의 구조 
입력층과 출력층을 가짐 ....

퍼셉트론의 동작
해당하는 특징값과 가중치를 곱한 결과를 모두 더하여 s를 구하고, 활성함수를 적용함

퍼셉트론은 선형 분류기라는 한계 존재 => 다층 퍼셉트론 개발
특징공간변환 => 퍼셉트론 2개를 사용한 XOR문제의 해결(퍼셉트론 1개를 순차 결합~ 다층 퍼셉트론)

다층 퍼셉트론의 특성
오류 역전파 알고리즘의 빠른 속도
모든 함수를 정확하게 근사할 수 있는 능력(은닉 노드를 무수히 많게 할 수 없으므로, 실직적으로는 복잡한 구조의 데이터에서는 성능 한계)
성능 향상을 위한 휴리스틱의 중요성 => 아키텍쳐: 은닉층과 은닉 노드의 개수 정함 / 초기값: 알고리즘에서의 가중치 초기화 / 학습률: 큰값으로 시작해 점점 줄이는 방식 / 활성홤수: ReLu
한계: 잡음이 섞인 상황에서 음성인식 성능 저하, 필기 주소 인식 능력 저하, 바둑에서의 한계
딥러닝은 이들 한계를 극복함

-----------------------------------------------------------------------------------------------------------------
softmax활성함수 => max를 모방
로그우도 목적함수 => oy라는 하나의 노드만 사용(MSE나 교차 엔트로피는 모든 출력 노드값)

softmax와 로그우도
softmax는 최대값이 아닌 값을 억제하여 0에 가깝게 만든다는 의도 내포
학습 샘플이 알려주는 부류에 해당하는 노드만 보겠다는 로그우도와 잘 어울림

최적화 기법 - 경사 하강법
오차 곡면이 최적해를 중심으로 하는 동심원 모양일 경우, 오차를 표현하는 손실 함수 곡면의 기울기의 반대방향은 언제나 최적해를 정확히 가리킴
모멘텀을 사용하는 방법 => 지금까지 기울기를 따라 이동한 경로를 누적한 모멘텀을 정의하고 이를 이용해 파라미터를 갱신

성능향상을 위한 요령
1.데이터 전처리 => 정규화(ex->평균이 0이되도록, 표준편차가 1이 되도록) / 명칭값을 원핫코드로 변환
2.가중치 초기화 => 대칭적 가중치 문제(두 노드가 같은 일을 하는 중복성 발생 => 난수를 초기화함으로써 대칭 파괴, 바이어스는 보통 0으로 초기화)
3.모멘텀 => 그레이디언트의 잡음 현상(그레디언트의 스무딩을 가하여 잡음 효과 줄임 => 수렴 속도 향상) / 효과: 오버슈팅 현상 누그러뜨림
네스테로프 모멘텀: 손실 함수 곡면의 기울기를 계산할 때 이전 모멘텀만큼 이동한 곳에서 기울기를 측정하여 이 기울기와 모멘텀을 함께 고려하여 최종적으로 이동할 곳을 찾는 것
4.적응적 학습률: 학습률이 너무 크면 오버슈팅에 따른 진자 현상, 너무 작으면 수렴이 느림 / 적응적 학습률은 매개변수마다 자신의 상황에 따라 학습률을 조절해 사용
적응적 경사 기법인 AdaGrad (길게 늘어나 있는 축 방향으로 손실 함수 곡면의 크기를 줄여주면 더 좋은 수렴 성능을 보일 수 있을 것)(누적기울기가 높으면 학습율 감소)(누적기울기는 지속적으로 증가)
RMSProp => AdaGrad를 해결하기 위해(값이 너무커지는 문제) => 간단히 이전 누적기울기가 일정한 비율로 쇠퇴하게 만들고 새로운 값이 쇠퇴한 만큼 초가하도록 하는 것으로 쇠퇴비율은 보통 0.9로 사용
적응적 모멘텀 추정 기법 => 모멘텀 최적화와 RMSProp을 섞어서 사용 일반적으로 Adam이라 부름
5.활성함수 => 활성값을 계산하고 활성함수를 적용하는 과정 / 시그모이드는 잘못씀 미분치가 0에 가깝게~ , 그래서 tanh씀(은닉층의 결과가 0인 상태에 도달해도 오차를 잘 전파)
(활성값이 커지면 포화 상태가 되고 그레디언트는 0에 가까워짐) => 매개변수 갱신(학습)이 매우 느린 요인
ReLu => 포화 문제 해소
6.배치 정규화 => 공변량 시프트 현상(자신에게 입력되는 데이터의 분포가 수시로 바뀜) => 공변량 시프트 현상을 누그러뜨리기 위해 정규화를 모든 층에 적용하는 기법

과잉적합에 빠지는 이유와 과잉적합을 피하는 전략
현대 기계 학습의 전략 => 충분히 큰 용량의 모델을 설계한 다음, 학습 과정에서 여러 규제 기법 적용
규제의 정의 => 현대 기계 학습도 매끄러움 가정을 널리 사용함

규제기법

명시적 규제 => 가중치 감쇠나 드롭아웃처럼 목적함수나 신경망 구조를 직접 수정하는 방식
암시적 규제 => 조기멈춤, 데이터 증대, 잡음 추가, 앙상블처럼 간접적으로 영향을 미치는 방식

1.가중치 벌칙 => 최종해를 원점 가까이 당기는 효과(즉 가중치를 작게 유지함) / 규제항 R로 L2놈을 사용하는 규제 기법을 가중치 감쇠라 부름
2.조기멈춤 => 검증집합의 오류가 최저인 점에서 학습을 멈춤
3.데이터 확대 => 데이터를 인위적으로 변형하여 확대함 / 한계:수작업 변형, 모든 부류가 같은 변형 사용 / 잡음을 섞어 확대하는 기법등이있음
4.드롭아웃 => 입력층과 은닉층의 노드 중 일정 비율을 임의로 선택하여 제거 / 많은 부분 신경망을 학습하고, 저장하고, 앙상블 결합하는 데 따른 계산 시간과 메모리 공간 측면의 부담
실제로는 가중치 공유 사용(하나의 신경망에 드롭아웃 적용)
5.앙상블 기법 => 서로 다른 여러 개의 모델을 결합해 일반화 오류를 줄이는 기법
(1) 서로 다른 예측기를 학습하는 일 (2) 학습된 예측기를 결합하는 일

